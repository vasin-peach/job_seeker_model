{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from csv import writer\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init it_industry word for search\n",
    "it_industry = pd.read_csv(\"data/jobs_position_category.csv\", header=None, encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init indeed\n",
    "base_url = \"https://www.indeed.com/\"\n",
    "pages = 30 # scape 5 page each page include 10 job = 300job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list_as_row(file_name, item):\n",
    "    with open(file_name, \"a+\") as file_object:\n",
    "        # Move read cursor to the start of file.\n",
    "        file_object.seek(0)\n",
    "        # If file is not empty then append '\\n'\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0 :\n",
    "            file_object.write(\"\\n\")\n",
    "        # Append text at the end of file\n",
    "        file_object.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_query(page):\n",
    "    response = requests.get(page)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_query_soup(keyword, page):\n",
    "    query_search = f\"{base_url}jobs?q={urllib.parse.quote(keyword)}&start={page}\"\n",
    "    return request_query(query_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(document):\n",
    "#     document = re.sub(r'\\W', ' ', str(document))\n",
    "    \n",
    "#     # remove all single characters\n",
    "#     document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "#     # remove single characters from the start\n",
    "#     document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "#     # substituting multiple spaces with single space\n",
    "#     document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "#     # remove hashtag #\n",
    "#     document = re.sub(r'#','',document) \n",
    "    \n",
    "#     # Converting to Lowercase\n",
    "#     document = document.lower()\n",
    "    \n",
    "#     return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_word(document):\n",
    "#     tokens = word_tokenize(clean_data(document))\n",
    "#     tokens = [i for i in tokens if not i in stop_words] # remove stop word\n",
    "#     tokens = [word for word in tokens if word.isalpha()] # remove alpha\n",
    "#     tokens = [i for i in tokens if not i.isnumeric()] # remove number\n",
    "#     tokens = [i for i in tokens if not ' ' in i] # remove space\n",
    "#     tokens = [stemmer.lemmatize(i) for i in tokens] # root\n",
    "    \n",
    "#     document = ' '.join(tokens)\n",
    "#     return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = tqdm(total=len(it_industry) * pages)\n",
    "# loop each page\n",
    "def run_scap(keyword, pages=1):\n",
    "    link_list = []\n",
    "    keyword_list = []\n",
    "    print(keyword)\n",
    "    for page in range(pages):\n",
    "        page = page * 10\n",
    "        soup = request_query_soup(keyword, page)\n",
    "\n",
    "        query_div = soup.find_all(\"div\", {\"class\": \"jobsearch-SerpJobCard\"})\n",
    "        for div in query_div:\n",
    "            link_list.append(f\"https://www.indeed.com/viewjob?jk={div.attrs['data-jk']}&from=serp&vjs=3\")\n",
    "            keyword_list.append(keyword)\n",
    "            \n",
    "    return link_list, keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(keyword):\n",
    "    return keyword.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  4\n"
     ]
    }
   ],
   "source": [
    "# number of core\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "\n",
    "# init pool\n",
    "pool = mp.Pool(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [pool.apply(test1, args =(keyword)) for keyword in it_industry[0]]\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_list, keyword_list = run_scap(it_industry[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0    Mobile Application Developer\n",
       " 1          Database Administrator\n",
       " 2                   Web Developer\n",
       " 3            Help Desk Technician\n",
       " Name: 0, dtype: object, 4    Network Administrator\n",
       " 5         Graphic Designer\n",
       " 6              IT Security\n",
       " 7        Software Engineer\n",
       " Name: 0, dtype: object, 8                          IT Manager\n",
       " 9                     Systems Analyst\n",
       " 10                     Data Scientist\n",
       " 11    Chief Information Officer (CIO)\n",
       " Name: 0, dtype: object, 12          Devops Engineer\n",
       " 13           Cloud Engineer\n",
       " 14    Business Intelligence\n",
       " Name: 0, dtype: object]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split = np.array_split(it_industry[0], mp.cpu_count(), axis=0)\n",
    "df_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mobile Application Developer',\n",
       "  'Database Administrator',\n",
       "  'Web Developer',\n",
       "  'Help Desk Technician'],\n",
       " ['Network Administrator',\n",
       "  'Graphic Designer',\n",
       "  'IT Security',\n",
       "  'Software Engineer'],\n",
       " ['IT Manager',\n",
       "  'Systems Analyst',\n",
       "  'Data Scientist',\n",
       "  'Chief Information Officer (CIO)'],\n",
       " ['Devops Engineer', 'Cloud Engineer', 'Business Intelligence']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[keyword.tolist() for keyword in df_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list, keyword_list = pool.map(run_scap, [keyword for keyword in it_industry[0]])\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load job_link\n",
    "job_link = pd.read_csv(\"indeed-scaper/data/job_url.csv\", header=None)\n",
    "job_data = pd.read_csv(\"indeed-scaper/data/job_scrap.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4425, 2)\n",
      "(4425, 3)\n"
     ]
    }
   ],
   "source": [
    "print(job_link.shape)\n",
    "print(job_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = job_data.drop(columns=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_new = pd.concat([job_link, job_data], axis=1)\n",
    "job_data_new\n",
    "job_data_new.columns=['job_type', 'link', 'job_title', 'company', 'desc']\n",
    "job_data_new.dropna(subset = ['job_title', 'desc'], inplace=True)\n",
    "job_data_new.to_csv('data/job_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5340/5340 [2:01:13<00:00,  1.36s/it]  \n"
     ]
    }
   ],
   "source": [
    "# loof each link\n",
    "header_list = []\n",
    "company_list = []\n",
    "desc_list = []\n",
    "job_link_temp = job_link[1]\n",
    "\n",
    "with tqdm(total=len(job_link_temp)) as pbar:\n",
    "    for link in job_link_temp:\n",
    "        soup = request(link)\n",
    "        header = soup.find(\"h3\", {\"class\": \"jobsearch-JobInfoHeader-title\"})\n",
    "        header = header.getText() if header else \"\"\n",
    "        company = soup.find(\"div\", {\"class\": \"jobsearch-InlineCompanyRating\"})\n",
    "        company = company.getText() if company else \"\"\n",
    "        desc = soup.find(\"div\", {\"class\": \"jobsearch-jobDescriptionText\"})\n",
    "#         desc = split_word(desc.getText()) if desc else \"\"\n",
    "\n",
    "        df = pd.DataFrame({'job_title': [header], 'company': [company], 'desc': [desc]})\n",
    "        df.to_csv('data/job_scrap.csv', mode='a', header=False, index=None)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_scrap = pd.read_csv(\"data/job_scrap.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job_scrap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-d374905adbac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjob_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob_link\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_scrap\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mjob_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'job_type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'link'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'job_title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'company'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'desc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mjob_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'job_title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'desc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mjob_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/job_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'job_scrap' is not defined"
     ]
    }
   ],
   "source": [
    "job_data = pd.concat([job_link, job_scrap], axis=1)\n",
    "job_data.columns=['job_type', 'link', 'job_title', 'company', 'desc']\n",
    "job_data.dropna(subset = ['job_title', 'desc'], inplace=True)\n",
    "job_data.to_csv('data/job_data.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
